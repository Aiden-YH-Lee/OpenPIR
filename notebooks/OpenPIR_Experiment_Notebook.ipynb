{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "K80Wu-cIcA0m"
   },
   "source": [
    "# OpenPIR: An Open-Source Dataset for Predominant Instrument Classification\n",
    "\n",
    "## Introduction\n",
    "\n",
    "This notebook contains the full experiment code for our OpenPIR paper. OpenPIR is a new dataset for predominant instrument classification. Previously, IRMAS was the only sufficient sized dataset available for this task, but it had issues such as unbalance across classes or training data being single-label while testing multi-label. We mainly reimplemented the experiment methodologies of the han paper https://arxiv.org/pdf/1605.09507. Using the experiment framework, we train the same model on our new dataset and test it.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "EHY1bptYub9v"
   },
   "source": [
    "## Imports: This section contains necessary imports and label dictionaries to map string labels to integers for training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "EDgEFhZng2ez"
   },
   "outputs": [],
   "source": [
    "# All import statements\n",
    "!pip install librosa\n",
    "\n",
    "# Standard library imports\n",
    "from collections import defaultdict\n",
    "from itertools import filterfalse\n",
    "import glob\n",
    "import json\n",
    "import logging\n",
    "import math\n",
    "import os\n",
    "import random\n",
    "\n",
    "# Third-party imports\n",
    "import librosa\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pickle\n",
    "import seaborn as sns\n",
    "from sklearn.metrics import f1_score, hamming_loss, classification_report, multilabel_confusion_matrix\n",
    "from sklearn.model_selection import train_test_split\n",
    "import torch\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "import torch.nn.init as init\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader, Sampler, Subset\n",
    "import torchaudio\n",
    "from torchvision import transforms\n",
    "from tqdm import tqdm\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# line wrap function for visual clarity\n",
    "from IPython.display import HTML, display\n",
    "\n",
    "def set_css():\n",
    "  display(HTML('''\n",
    "  <style>\n",
    "    pre {\n",
    "        white-space: pre-wrap;\n",
    "    }\n",
    "  </style>\n",
    "  '''))\n",
    "get_ipython().events.register('pre_run_cell', set_css)\n",
    "\n",
    "# mount drive\n",
    "from google.colab import drive\n",
    "drive.mount('/content/drive')\n",
    "\n",
    "folder_path = '/content/drive/MyDrive/IRMAS-TrainingData'\n",
    "\n",
    "os.listdir(folder_path)\n",
    "\n",
    "# need this for computer to map integer values to labels we use\n",
    "instrument_labels = {'cel': 0, 'cla': 1, 'flu': 2, 'gac': 3, 'gel': 4, 'org': 5, 'pia': 6, 'sax': 7, 'tru': 8, 'vio': 9, 'voi': 10}\n",
    "\n",
    "openmic_instrument_labels = {'cel': 0, 'cla': 1, 'flu': 2, 'guitar': 3, 'org': 4, 'pia': 5, 'sax': 6, 'tru': 7, 'vio': 8, 'voi': 9}\n",
    "\n",
    "# instrument_labels = {'cello': 0, 'clarinet': 1, 'flute': 2, 'acoustic guitar': 3, 'electric guitar': 4, 'organ': 5, 'piano': 6, 'saxophone': 7, 'trumpet': 8, 'violin': 9, 'voice': 10}\n",
    "IRMAS_genre_labels = {'cla': 0, 'pop_roc': 1, 'cou_fol': 2, 'jaz_blu': 3, 'lat_sou': 4}\n",
    "\n",
    "# {'pop_roc', 'jaz_blu', 'lat_sou', 'cou_fol', 'cla'}\n",
    "lastfm_genre_labels = {\n",
    "    'blues': 0,\n",
    "    'classical': 1,\n",
    "    'country': 2,\n",
    "    'disco': 3,\n",
    "    'jazz': 3,\n",
    "    'folk': 4,\n",
    "    'metal': 5,\n",
    "    'pop': 6,\n",
    "    'rock': 7,\n",
    "    'latin': 8,\n",
    "    'soul': 9\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 53
    },
    "id": "wbJj2nxbwPdf",
    "outputId": "626f10b6-f0ef-4ef2-d64e-b26a51177f63"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "  <style>\n",
       "    pre {\n",
       "        white-space: pre-wrap;\n",
       "    }\n",
       "  </style>\n",
       "  "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "  <style>\n",
       "    pre {\n",
       "        white-space: pre-wrap;\n",
       "    }\n",
       "  </style>\n",
       "  "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing entries: 100%|██████████| 3174/3174 [40:05<00:00,  1.32it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved 98866 dictionaries to /content/drive/MyDrive/Data/IRMAS/IRMASG_TEST_FILTERED_NEWSAMPLES.pkl\n"
     ]
    }
   ],
   "source": [
    "# @title Audio Feature Extraction and Preprocessing:\n",
    "\n",
    "# This section extracts audio features (mel-spectrogram, MFCCs, etc.) from audio files, performs preprocessing (mono conversion, normalization, chunking), and saves the processed data.\n",
    "\n",
    "\n",
    "\n",
    "# def chunk_audio(waveform, chunk_length=1.0, overlap=0.0, sr=22050):\n",
    "#     \"\"\"\n",
    "#     1) If stereo, convert to mono by averaging channels.\n",
    "#     2) Normalize amplitude to [-1,1].\n",
    "#     3) Split waveform into overlapping chunks of length `chunk_length` seconds.\n",
    "#     \"\"\"\n",
    "#     if waveform.ndim == 1:\n",
    "#         waveform = waveform.unsqueeze(0)\n",
    "#     # If stereo => average over channel dimension => [1, T]\n",
    "#     if waveform.ndim == 2 and waveform.shape[0] > 1:\n",
    "#         waveform = torch.mean(waveform, dim=0, keepdim=True)\n",
    "\n",
    "#     # Normalize\n",
    "#     max_val = waveform.abs().max()\n",
    "#     if max_val > 0:\n",
    "#         waveform = waveform / max_val\n",
    "\n",
    "#     chunk_size = int(sr * chunk_length)\n",
    "#     stride = int(chunk_size * (1 - overlap))\n",
    "#     num_chunks = (waveform.shape[1] - chunk_size) // stride + 1\n",
    "#     num_chunks = max(num_chunks, 1)  # at least 1 chunk\n",
    "\n",
    "#     chunks = []\n",
    "#     start = 0\n",
    "#     for i in range(num_chunks):\n",
    "#         end = start + chunk_size\n",
    "#         if end >= waveform.shape[1]:\n",
    "#             # If the last chunk is partial, slice from the back\n",
    "#             chunk = waveform[:, -chunk_size:]\n",
    "#             chunks.append(chunk)\n",
    "#             break\n",
    "#         else:\n",
    "#             chunk = waveform[:, start:end]\n",
    "#             chunks.append(chunk)\n",
    "#         start += stride\n",
    "\n",
    "#     return chunks\n",
    "\n",
    "\n",
    "# def compute_librosa_feats(chunk, sr=22050):\n",
    "#     \"\"\"\n",
    "#     Extract various librosa features from a single chunk.\n",
    "#     `chunk` has shape [1, chunk_size].\n",
    "#     \"\"\"\n",
    "#     # Remove channel dimension => shape [chunk_size]\n",
    "#     waveform_np = chunk.squeeze(0).cpu().numpy()\n",
    "\n",
    "#     zcr = librosa.feature.zero_crossing_rate(waveform_np, frame_length=1024, hop_length=512)[0]\n",
    "#     centroid = librosa.feature.spectral_centroid(y=waveform_np, sr=sr, n_fft=1024, hop_length=512)[0]\n",
    "#     rms = librosa.feature.rms(y=waveform_np, frame_length=1024, hop_length=512)[0]\n",
    "#     rolloff = librosa.feature.spectral_rolloff(y=waveform_np, sr=sr, n_fft=1024, hop_length=512)[0]\n",
    "#     bandwidth = librosa.feature.spectral_bandwidth(y=waveform_np, sr=sr, n_fft=1024, hop_length=512)[0]\n",
    "\n",
    "#     mel_spec = librosa.feature.melspectrogram(\n",
    "#         y=waveform_np, sr=sr, n_fft=1024, hop_length=512,\n",
    "#         n_mels=128, power=2.0\n",
    "#     )\n",
    "#     mel_db = librosa.power_to_db(mel_spec)\n",
    "\n",
    "#     mfcc = librosa.feature.mfcc(\n",
    "#         y=waveform_np, sr=sr, n_mfcc=20,\n",
    "#         n_fft=1024, hop_length=512\n",
    "#     )\n",
    "\n",
    "#     return {\n",
    "#         \"zcr\": zcr,\n",
    "#         \"centroid\": centroid,\n",
    "#         \"rms\": rms,\n",
    "#         \"rolloff\": rolloff,\n",
    "#         \"bandwidth\": bandwidth,\n",
    "#         \"mel\": mel_db,\n",
    "#         \"mfcc\": mfcc\n",
    "#     }\n",
    "\n",
    "\n",
    "# def process_and_save(data, output_file, chunk_length=1.0,\n",
    "#                      overlap=0.0, sr=22050):\n",
    "#     \"\"\"\n",
    "#     - list_of_dicts: each entry has something like:\n",
    "#         {\n",
    "#           'filepath': '...',\n",
    "#           'audio': tensor([...]),  # shape [2, T] or [1, T]\n",
    "#           'instrument': 'electric guitar',\n",
    "#           'genre': 'pop_roc',\n",
    "#           ...\n",
    "#         }\n",
    "#     - We'll split each 'audio' into 1-sec chunks, compute features,\n",
    "#       and produce multiple new dictionaries, one for each chunk.\n",
    "#     - 'audio' in the new dict => the feature dictionary from compute_librosa_feats\n",
    "#     - Add 'index' key => chunk number.\n",
    "#     - Keep all other keys the same (filepath, instrument, etc.).\n",
    "#     - Write the final list to `output_file` as a pickle.\n",
    "#     \"\"\"\n",
    "#     list_of_dicts = torch.load(data, weights_only=False)\n",
    "#     processed_list = []\n",
    "\n",
    "#     for entry in tqdm(list_of_dicts, desc=\"Processing entries\"):\n",
    "#         audio_data = entry['audio']\n",
    "#         # If 'audio' is numpy, convert to torch\n",
    "#         if isinstance(audio_data, np.ndarray):\n",
    "#             audio_data = torch.from_numpy(audio_data)\n",
    "\n",
    "#         # Chunk the audio\n",
    "#         chunks = chunk_audio(audio_data, chunk_length=chunk_length,\n",
    "#                              overlap=overlap, sr=sr)\n",
    "\n",
    "#         # For each chunk, compute features & produce a new dictionary\n",
    "#         for i, ch in enumerate(chunks):\n",
    "#             feats = compute_librosa_feats(ch, sr=sr)\n",
    "\n",
    "#             # Copy original dict\n",
    "#             new_dict = dict(entry)\n",
    "#             # Replace 'audio' with the feature dictionary\n",
    "#             new_dict[\"audio\"] = feats\n",
    "#             # Add chunk index\n",
    "#             new_dict[\"index\"] = i\n",
    "\n",
    "#             processed_list.append(new_dict)\n",
    "\n",
    "#     torch.save(processed_list, output_file)\n",
    "\n",
    "#     print(f\"Saved {len(processed_list)} dictionaries to {output_file}\")\n",
    "\n",
    "\n",
    "# # Example usage:\n",
    "# # Suppose you have a list_of_dicts loaded from somewhere, e.g.:\n",
    "# # list_of_dicts = [\n",
    "# #   {'filepath': '/some/path.wav', 'audio': torch.randn(2, 44100), 'instrument': 'guitar', ...},\n",
    "# #   {'filepath': '/some/other.wav', 'audio': torch.randn(2, 88200), 'instrument': 'voice', ...},\n",
    "# #   ...\n",
    "# # ]\n",
    "# #\n",
    "# process_and_save(\"/content/drive/MyDrive/Data/IRMAS/filtered_dataset_newsamples.pt\", \"/content/drive/MyDrive/Data/IRMAS/IRMASG_TEST_FILTERED_NEWSAMPLES.pkl\", chunk_length=1.0, overlap=0.5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6vYMQZYs0k4L"
   },
   "source": [
    "## Helper functions:\n",
    "\n",
    "We also experimented with genre information but did not end up using them in the end."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 17
    },
    "id": "HwLvevhomCp-",
    "outputId": "6f9b5398-1f35-4882-dd8f-4cf3a266411d"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "  <style>\n",
       "    pre {\n",
       "        white-space: pre-wrap;\n",
       "    }\n",
       "  </style>\n",
       "  "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "  <style>\n",
       "    pre {\n",
       "        white-space: pre-wrap;\n",
       "    }\n",
       "  </style>\n",
       "  "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "def one_hot_encode_genre(d, lastfm_genre_labels, length):\n",
    "    # Create a one-hot vector of length equal to the number of lastfm genres.\n",
    "    num_genres = length\n",
    "    one_hot = np.zeros(num_genres, dtype=int)\n",
    "\n",
    "    # Determine which genre information to use.\n",
    "    # Use genre_list and/or genre_lastfm if available.\n",
    "    genres = []\n",
    "    if d.get(\"openmic\") == True:\n",
    "      if d.get(\"IRMASG_genres\") and len(d[\"IRMASG_genres\"]) > 0:\n",
    "          genres.extend(d[\"IRMASG_genres\"])\n",
    "    else:\n",
    "      if d.get(\"genre_list\") and len(d[\"genre_list\"]) > 0:\n",
    "          genres.extend(d[\"genre_list\"])\n",
    "      if d.get(\"genre_lastfm\") is not None:\n",
    "          genres.append(d[\"genre_lastfm\"])\n",
    "\n",
    "    # If both are empty, fall back on d[\"genre\"] which might be a combined string (e.g., \"pop_roc\").\n",
    "    if not genres:\n",
    "        # Split the combined string on the underscore.\n",
    "        irmas_genre = d[\"genre\"]\n",
    "        if irmas_genre == 'pop_roc':\n",
    "            genres = ['pop', 'rock']\n",
    "        elif irmas_genre == 'lat_sou':\n",
    "            genres = ['latin', 'soul']\n",
    "        elif irmas_genre == 'cou_fol':\n",
    "            genres = ['country', 'folk']\n",
    "        elif irmas_genre == 'jaz_blu':\n",
    "            genres = ['jazz', 'blues']\n",
    "        elif irmas_genre == 'cla':\n",
    "            genres = ['classical']\n",
    "        else:\n",
    "            genres = [irmas_genre]\n",
    "\n",
    "    # One-hot encode each genre present.\n",
    "    for g in genres:\n",
    "        if g in lastfm_genre_labels:\n",
    "            idx = lastfm_genre_labels[g]\n",
    "            one_hot[idx] = 1\n",
    "        else:\n",
    "            if g == None:\n",
    "              # print(d)\n",
    "            # print(f\"Warning: Genre '{g}' not found in mapping.\")\n",
    "              pass\n",
    "\n",
    "    if(np.all(one_hot == 0)):\n",
    "      # print(\"All zeros: \", d)\n",
    "      pass\n",
    "\n",
    "    return one_hot\n",
    "\n",
    "# # Example usage:\n",
    "# lastfm_genre_labels = {\n",
    "#     'blues': 0,\n",
    "#     'classical': 1,\n",
    "#     'country': 2,\n",
    "#     'disco': 3,\n",
    "#     'jazz': 3,\n",
    "#     'folk': 4,\n",
    "#     'metal': 5,\n",
    "#     'pop': 6,\n",
    "#     'rock': 7,\n",
    "#     'latin': 8,\n",
    "#     'soul': 9\n",
    "# }\n",
    "\n",
    "# # Data dictionary example:\n",
    "# data_dict = {\n",
    "#     \"genre\": \"pop_roc\",\n",
    "#     \"genre_list\": [],        # or e.g., [\"rock\", \"metal\"]\n",
    "#     \"genre_lastfm\": None     # or e.g., \"rock\"\n",
    "# }\n",
    "\n",
    "# one_hot_vector = one_hot_encode_genre(data_dict, lastfm_genre_labels)\n",
    "# print(one_hot_vector)\n",
    "def multi_hot_encode_instruments(d, instrument_labels):\n",
    "    num_instruments = len(instrument_labels)\n",
    "    one_hot = np.zeros(num_instruments, dtype=int)\n",
    "\n",
    "    # instruments = d.get(\"instrument\", [])\n",
    "    instruments = d\n",
    "\n",
    "    if isinstance(instruments, str):\n",
    "        instruments = [instruments]  # Single string converted to list\n",
    "\n",
    "    if instruments is None or len(instruments) == 0:\n",
    "        return one_hot  # return all zeros if no instruments are provided\n",
    "\n",
    "    for inst in instruments:\n",
    "        if inst in instrument_labels:\n",
    "            idx = instrument_labels[inst]\n",
    "            one_hot[idx] = 1\n",
    "        else:\n",
    "            # print(f\"Warning: Instrument '{inst}' not found in mapping.\")\n",
    "            pass\n",
    "\n",
    "    return one_hot\n",
    "\n",
    "def compute_mean_std(dataset):\n",
    "    \"\"\"\n",
    "    Compute the global per-channel mean and standard deviation for a dataset.\n",
    "    Assumes that each __getitem__ returns a tensor of shape [1, C, frames].\n",
    "    \"\"\"\n",
    "    sum_ = None\n",
    "    sumsq_ = None\n",
    "    count = 0\n",
    "\n",
    "    for i in range(len(dataset)):\n",
    "        features, _, _ = dataset[i]  # features shape: [1, C, frames]\n",
    "        features = features.squeeze(0)  # now shape: [C, frames]\n",
    "        if sum_ is None:\n",
    "            sum_ = features.sum(dim=1)\n",
    "            sumsq_ = (features ** 2).sum(dim=1)\n",
    "            count = features.shape[1]\n",
    "        else:\n",
    "            sum_ += features.sum(dim=1)\n",
    "            sumsq_ += (features ** 2).sum(dim=1)\n",
    "            count += features.shape[1]\n",
    "\n",
    "    mean = sum_ / count\n",
    "    var = (sumsq_ / count) - (mean ** 2)\n",
    "    std = torch.sqrt(var.clamp_min(1e-8))\n",
    "    return mean, std\n",
    "\n",
    "\n",
    "# function to initialize weights of model using glorot as mentioned in paper\n",
    "def weights_init(m):\n",
    "    \"\"\"Apply Glorot (Xavier) initialization to all Conv2d and Linear layers.\"\"\"\n",
    "\n",
    "    if isinstance(m, nn.Conv2d) or isinstance(m, nn.Linear):\n",
    "        init.xavier_uniform_(m.weight)  # Apply Xavier Uniform\n",
    "        if m.bias is not None:\n",
    "            init.zeros_(m.bias)  # Bias initialized to zero\n",
    "\n",
    "    elif isinstance(m, Conv_2d):  # Handle your custom Conv_2d class\n",
    "        init.xavier_uniform_(m.conv1.weight)\n",
    "        if m.conv1.bias is not None:\n",
    "            init.zeros_(m.conv1.bias)\n",
    "\n",
    "        init.xavier_uniform_(m.conv2.weight)\n",
    "        if m.conv2.bias is not None:\n",
    "            init.zeros_(m.conv2.bias)\n",
    "\n",
    "    elif isinstance(m, nn.BatchNorm2d):  # Batch Norm initialization\n",
    "        init.ones_(m.weight)\n",
    "        init.zeros_(m.bias)\n",
    "\n",
    "\n",
    "def set_seed(seed=42):\n",
    "    \"\"\"Set seed for reproducibility across NumPy, PyTorch, and random operations.\"\"\"\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False  # Ensures consistency\n",
    "\n",
    "def seed_worker(worker_id):\n",
    "    \"\"\"Ensures DataLoader workers have consistent seeds.\"\"\"\n",
    "    worker_seed = torch.initial_seed() % 2**32\n",
    "    np.random.seed(worker_seed)\n",
    "    random.seed(worker_seed)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Zy9o0xnByyeD"
   },
   "source": [
    "## OpenPIRDataset:\n",
    "This class defines a custom PyTorch Dataset for loading and preprocessing audio data from the OpenPIR dataset. It handles tasks like loading audio features, encoding labels, splitting into train/validation sets, and applying normalization. We precomputed different features to try implementing methodologies from other papers but ended up only using melspec.\n",
    "\n",
    "**Key functionalities:**\n",
    "\n",
    "* **Initialization (`__init__`)**:\n",
    "  - Loads audio features and labels from a specified data file (`.pkl` or `.npy`).\n",
    "  - Encodes instrument and genre labels using `multi_hot_encode_instruments` and `one_hot_encode_genre`.\n",
    "  - Splits the data into train/validation sets based on the `split` argument.\n",
    "  - Stores normalization parameters (mean and standard deviation) if provided.\n",
    "* **Data Access (`__getitem__`)**:\n",
    "  - Retrieves the audio features, instrument labels, and genre labels for a given index.\n",
    "  - Converts the features to PyTorch tensors.\n",
    "  - Applies normalization if mean and standard deviation are available.\n",
    "  - Returns the processed data.\n",
    "* **Dataset Length (`__len__`)**:\n",
    "  - Returns the total number of samples in the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 17
    },
    "id": "5hpkVQRMfur7",
    "outputId": "d338ae38-b184-477a-918e-b0359792ae41"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "  <style>\n",
       "    pre {\n",
       "        white-space: pre-wrap;\n",
       "    }\n",
       "  </style>\n",
       "  "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "  <style>\n",
       "    pre {\n",
       "        white-space: pre-wrap;\n",
       "    }\n",
       "  </style>\n",
       "  "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "class OpenPIRDataset(Dataset):\n",
    "    def __init__(self, data_file, split='all', test_sample_half=False, seed=42, mean=None, std=None, openmic=True):\n",
    "        \"\"\"\n",
    "        Initializes the OpenPIRDataset.\n",
    "\n",
    "        Args:\n",
    "            data_file (str): Path to the data file (.pkl or .npy).\n",
    "            split (str, optional): Data split ('train', 'valid', 'all'). Defaults to 'all'.\n",
    "            test_sample_half (bool, optional): Whether to sample half of the test data. Defaults to False.\n",
    "            seed (int, optional): Random seed for splitting. Defaults to 42.\n",
    "            mean (torch.Tensor, optional): Mean for normalization. Defaults to None.\n",
    "            std (torch.Tensor, optional): Standard deviation for normalization. Defaults to None.\n",
    "            openmic (bool, optional): Whether the dataset is OpenMIC. Defaults to True.\n",
    "        \"\"\"\n",
    "        # Load data based on file type\n",
    "        if data_file.endswith('.pkl'):\n",
    "            with open(data_file, 'rb') as f:\n",
    "                data_list = torch.load(f, weights_only=False)\n",
    "        elif data_file.endswith('.npy'):\n",
    "            data_list = np.load(data_file, allow_pickle=True)\n",
    "        else:\n",
    "            raise ValueError(\"Unsupported file format: must be .pkl or .npy\")\n",
    "\n",
    "        # Ensure consistent instrument key\n",
    "        for d in data_list:\n",
    "            if \"instrument\" not in d and \"instruments\" in d:\n",
    "                d[\"instrument\"] = d[\"instruments\"]\n",
    "\n",
    "        # Filter out irrelevant entries\n",
    "        data_list = [\n",
    "            d for d in data_list\n",
    "            if \"instrument\" in d and d[\"instrument\"] is not None and\n",
    "            not (isinstance(d[\"instrument\"], list) and len(d[\"instrument\"]) == 1 and d[\"instrument\"][0] == \"other\")\n",
    "        ]\n",
    "\n",
    "        # Extract features, instrument, genre, and audio indices\n",
    "        all_features = [d[\"audio\"] for d in data_list]\n",
    "        all_instrument = [multi_hot_encode_instruments(d, instrument_labels) for d in data_list]  # Use appropriate labels based on openmic\n",
    "        all_genre = [one_hot_encode_genre(d, lastfm_genre_labels, 10) for d in data_list]\n",
    "        filepaths = [d[\"filepath\"] for d in data_list]\n",
    "        unique_paths = list(set(filepaths))\n",
    "        path2idx = {p: i for i, p in enumerate(unique_paths)}\n",
    "        audio_idx = [path2idx[p] for p in filepaths]\n",
    "\n",
    "        # Split data if necessary\n",
    "        if split in ['train', 'valid']:\n",
    "            indices = np.arange(len(data_list))\n",
    "            train_idx, val_idx = train_test_split(\n",
    "                indices,\n",
    "                test_size=0.15,\n",
    "                random_state=seed,\n",
    "                stratify=all_instrument  # Uncomment for stratification if desired.\n",
    "            )\n",
    "            selected_indices = train_idx if split == 'train' else val_idx\n",
    "        else:\n",
    "            selected_indices = np.arange(len(data_list))\n",
    "\n",
    "        # Store selected data\n",
    "        self.features = [all_features[i] for i in selected_indices]\n",
    "        self.instrument = [all_instrument[i] for i in selected_indices]\n",
    "        self.genre = [all_genre[i] for i in selected_indices]\n",
    "        self.audio_idx = [audio_idx[i] for i in selected_indices]\n",
    "        self.split = split\n",
    "        self.mean = mean\n",
    "        self.std = std\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.features)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        \"\"\"\n",
    "        Retrieves data for a given index.\n",
    "\n",
    "        Args:\n",
    "            idx (int): Index of the data sample.\n",
    "\n",
    "        Returns:\n",
    "            tuple: Audio features, instrument label, and genre/audio index.\n",
    "        \"\"\"\n",
    "        feats_dict = self.features[idx]\n",
    "        main_label = self.instrument[idx]\n",
    "        aux_label = self.genre[idx]\n",
    "        audio_idx_val = self.audio_idx[idx]\n",
    "\n",
    "        # Convert features to tensors\n",
    "        zcr_t = torch.from_numpy(feats_dict['zcr']).float()\n",
    "        centroid_t = torch.from_numpy(feats_dict['centroid']).float()\n",
    "        rms_t = torch.from_numpy(feats_dict['rms']).float()\n",
    "        rolloff_t = torch.from_numpy(feats_dict['rolloff']).float()\n",
    "        bandwidth_t = torch.from_numpy(feats_dict['bandwidth']).float()\n",
    "        mel_t = torch.from_numpy(feats_dict['mel']).float()\n",
    "        mfcc_t = torch.from_numpy(feats_dict['mfcc']).float()\n",
    "\n",
    "        # Combine features\n",
    "        scalar_stack = torch.stack([zcr_t, centroid_t, rms_t, rolloff_t, bandwidth_t], dim=0)\n",
    "        combined_features = mel_t.unsqueeze(0)  # Use only mel-spectrogram\n",
    "\n",
    "        # Apply normalization\n",
    "        if (self.mean is not None) and (self.std is not None):\n",
    "            mean_ = self.mean.view(1, -1, 1)\n",
    "            std_ = self.std.view(1, -1, 1)\n",
    "            combined_features = (combined_features - mean_) / (std_ + 1e-8)\n",
    "\n",
    "        # Return data based on split\n",
    "        if self.split in ['train', 'valid']:\n",
    "            return combined_features, main_label, aux_label\n",
    "        else:\n",
    "            return combined_features, main_label, audio_idx_val"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RcVyFtD73-c3"
   },
   "source": [
    "## IRMASGTestDataset:\n",
    "\n",
    "This class defines a custom PyTorch Dataset for loading and preprocessing audio data for testing, similar to `OpenPIRDataset`. It handles loading precomputed features, labels, and audio indices, and applies normalization if necessary. Note that this is the same as the original Test Dataset from IRMAS.\n",
    "\n",
    "**Key functionalities:**\n",
    "\n",
    "* **Initialization (`__init__`)**:\n",
    "    - Loads precomputed features, labels, and audio indices from a specified data file (`.pkl` or `.npy`).\n",
    "    - Stores normalization parameters (mean and standard deviation) if provided.\n",
    "* **Data Access (`__getitem__`)**:\n",
    "    - Retrieves the audio features, instrument labels, and audio index for a given index.\n",
    "    - Converts the features to PyTorch tensors.\n",
    "    - Applies normalization if mean and standard deviation are available.\n",
    "    - Returns the processed data.\n",
    "* **Dataset Length (`__len__`)**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 17
    },
    "id": "tVHzBKxPNioc",
    "outputId": "6a383676-dbc2-4256-95b9-05b7c238b52e"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "  <style>\n",
       "    pre {\n",
       "        white-space: pre-wrap;\n",
       "    }\n",
       "  </style>\n",
       "  "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "  <style>\n",
       "    pre {\n",
       "        white-space: pre-wrap;\n",
       "    }\n",
       "  </style>\n",
       "  "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "class IRMASGTestDataset(Dataset):\n",
    "    def __init__(self, data_file, split='train', val_size=0.15, test_sample_half=False, seed=42, aux_mapping={\n",
    "        '0': 1, '1': 1, '2': 1, '3': 0, '4': 0,\n",
    "        '5': 0, '6': 0, '7': 1, '8': 1, '9': 1, '10': 2\n",
    "    }, mean=None, std=None):\n",
    "        \"\"\"\n",
    "        data_file: path to the .pkl or .npy with keys [\"features\", \"labels\", \"audio_idx\"]\n",
    "        split: 'train', 'valid', or 'test'.\n",
    "        val_size: fraction of data to use for validation.\n",
    "        test_sample_half: whether to randomly sample half of the unique test audio indices.\n",
    "        seed: random seed for reproducibility.\n",
    "        aux_mapping: optional dictionary for auxiliary labels.\n",
    "        \"\"\"\n",
    "\n",
    "        # 1) Load the precomputed dictionary\n",
    "        if data_file.endswith('.pkl'):\n",
    "            with open(data_file, 'rb') as f:\n",
    "                data_dict = pickle.load(f)\n",
    "        elif data_file.endswith('.npy'):\n",
    "            data_dict = np.load(data_file, allow_pickle=True).item()\n",
    "        else:\n",
    "            raise ValueError(\"Unsupported file format: must be .pkl or .npy\")\n",
    "\n",
    "        features = data_dict['features']\n",
    "        labels = data_dict['labels']\n",
    "        audio_idx = data_dict['audio_idx']\n",
    "\n",
    "        # 2) Train-validation split (no test split here)\n",
    "        train_indices, val_indices = train_test_split(\n",
    "            np.arange(len(features)), test_size=val_size, random_state=seed, stratify=labels\n",
    "        )\n",
    "\n",
    "        if split == 'train':\n",
    "            selected_indices = train_indices\n",
    "        elif split == 'valid':\n",
    "            selected_indices = val_indices\n",
    "        elif split == 'test':\n",
    "            # Select test set (use all indices initially)\n",
    "            selected_indices = np.arange(len(features))\n",
    "\n",
    "            if test_sample_half:\n",
    "                # Find unique audio indices\n",
    "                unique_audio_indices = np.unique(audio_idx)\n",
    "\n",
    "                # Randomly select half of them\n",
    "                np.random.seed(seed)\n",
    "                sampled_audio_indices = np.random.choice(\n",
    "                    unique_audio_indices, size=len(unique_audio_indices) // 2, replace=False)\n",
    "\n",
    "                # Filter dataset to include only selected audio indices\n",
    "                selected_indices = [\n",
    "                    i for i in selected_indices if audio_idx[i] in sampled_audio_indices]\n",
    "\n",
    "        else:\n",
    "            raise ValueError(\"Invalid split. Use 'train', 'valid', or 'test'.\")\n",
    "\n",
    "        # 3) Apply the selected indices to filter data\n",
    "        self.features = [features[i] for i in selected_indices]\n",
    "        self.labels = [labels[i] for i in selected_indices]\n",
    "        self.audio_idx = [audio_idx[i] for i in selected_indices]\n",
    "        self.split = split\n",
    "        self.aux_mapping = aux_mapping\n",
    "\n",
    "        self.mean = mean\n",
    "        self.std = std\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.features)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        \"\"\"\n",
    "        Returns:\n",
    "          combined_features: shape [153, frames]\n",
    "          label: (optional) torch.LongTensor\n",
    "          audio_idx: (only if you want it, e.g. in test)\n",
    "        \"\"\"\n",
    "        feats_dict = self.features[idx]\n",
    "        label = self.labels[idx]\n",
    "        audio_idx = self.audio_idx[idx]\n",
    "\n",
    "        # Convert each numpy array to torch.Tensors\n",
    "        zcr_t = torch.from_numpy(feats_dict['zcr']).type(torch.float32)\n",
    "        centroid_t = torch.from_numpy(\n",
    "            feats_dict['centroid']).type(torch.float32)\n",
    "        rms_t = torch.from_numpy(feats_dict['rms']).type(torch.float32)\n",
    "        rolloff_t = torch.from_numpy(feats_dict['rolloff']).type(torch.float32)\n",
    "        bandwidth_t = torch.from_numpy(\n",
    "            feats_dict['bandwidth']).type(torch.float32)\n",
    "        mel_t = torch.from_numpy(feats_dict['mel']).type(torch.float32)\n",
    "        mfcc_t = torch.from_numpy(feats_dict['mfcc']).type(torch.float32)\n",
    "\n",
    "        # Stack the 5 scalar features into [5, frames]\n",
    "        scalar_stack = torch.stack(\n",
    "            [zcr_t, centroid_t, rms_t, rolloff_t, bandwidth_t], dim=0)\n",
    "\n",
    "        # Concatenate to get [153, frames]: 5 scalars + 128 mel + 20 mfcc\n",
    "        # combined_features = torch.cat([scalar_stack, mel_t, mfcc_t], dim=0).unsqueeze(0)\n",
    "        combined_features = mel_t.unsqueeze(0)\n",
    "\n",
    "        # Apply normalization if available\n",
    "        if (self.mean is not None) and (self.std is not None):\n",
    "            mean_ = self.mean.view(1, -1, 1)  # [1, C, 1]\n",
    "            std_ = self.std.view(1, -1, 1)\n",
    "            combined_features = (combined_features - mean_) / (std_ + 1e-8)\n",
    "\n",
    "        label_t = torch.tensor(label, dtype=torch.long)\n",
    "        if self.split in ['train', 'valid']:\n",
    "            aux_label = self.aux_mapping[str(label)]\n",
    "            aux_label_t = torch.tensor(aux_label, dtype=torch.long)\n",
    "            return combined_features, label_t, aux_label_t\n",
    "        else:\n",
    "            return combined_features, label_t, audio_idx"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0V58MAP3S_Th"
   },
   "source": [
    "## TestBatchSampler: Maintaining Audio Context During Testing\n",
    "\n",
    "The `TestBatchSampler` is a custom PyTorch sampler crucial for testing\n",
    "audio-based models. It ensures that all slices or chunks belonging to\n",
    "the same audio file are processed together in a batch or in controlled\n",
    "sub-batches. This is essential for preserving the temporal context of\n",
    "the audio, which is often vital for accurate predictions in tasks like\n",
    "instrument classification or music genre recognition. Essentially, it allows the model to process one full audio sample as one batch, so that we can know that all the 1-second snippets in this batch belong to the same audio, and hence aggregate evaluations.\n",
    "\n",
    "**How it Works:**\n",
    "\n",
    "1. **Initialization:** The sampler takes a dictionary `audio_to_indices`\n",
    "   that maps each unique audio ID to a list of corresponding dataset indices\n",
    "   representing its slices. It also takes the desired `batch_size`.\n",
    "\n",
    "2. **Batch Generation:** During iteration, the sampler retrieves all indices\n",
    "   associated with a single audio ID. It then yields either:\n",
    "   - All indices at once if the `batch_size` can accommodate them.\n",
    "   - Sub-batches of the specified size, ensuring that slices from the same\n",
    "     audio are grouped.\n",
    "\n",
    "3. **Batch Count:** The sampler calculates the total number of batches,\n",
    "   considering potential sub-batches created due to `batch_size` constraints.\n",
    "\n",
    "**Benefits:**\n",
    "\n",
    "- **Preserves Audio Context:** By processing all slices of an audio together,\n",
    "  the model can make more informed predictions based on the complete temporal\n",
    "  information.\n",
    "- **Controlled Batching:** The `batch_size` parameter allows flexibility in\n",
    "  managing batch sizes while ensuring that audio context is maintained.\n",
    "- **Improved Accuracy:** By preserving context, the sampler contributes to\n",
    "  achieving more accurate results in audio-based tasks.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 17
    },
    "id": "dN4stncWgJ68",
    "outputId": "1f617707-19ed-4521-dbfc-f14a16c79a6b"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "  <style>\n",
       "    pre {\n",
       "        white-space: pre-wrap;\n",
       "    }\n",
       "  </style>\n",
       "  "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "  <style>\n",
       "    pre {\n",
       "        white-space: pre-wrap;\n",
       "    }\n",
       "  </style>\n",
       "  "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "\n",
    "class TestBatchSampler(Sampler):\n",
    "    def __init__(self, audio_to_indices, batch_size):\n",
    "        \"\"\"\n",
    "        audio_to_indices: dict[int, list[int]]\n",
    "            A dict mapping audio_idx -> list of dataset indices\n",
    "            that correspond to that audio.\n",
    "        batch_size: int\n",
    "            Max number of samples in a batch. If you want to keep\n",
    "            each audio in exactly one batch, you can just set this\n",
    "            to a very large number or the max number of slices.\n",
    "        \"\"\"\n",
    "        self.audio_to_indices = audio_to_indices\n",
    "        self.audio_idxs = list(audio_to_indices.keys())\n",
    "        self.batch_size = batch_size\n",
    "\n",
    "    def __iter__(self):\n",
    "        \"\"\"\n",
    "        Yield a list of dataset indices in each batch.\n",
    "        In this case, we yield all items for one audio_idx\n",
    "        (possibly split if you want smaller batches).\n",
    "        \"\"\"\n",
    "        for audio_idx in self.audio_idxs:\n",
    "            indices = self.audio_to_indices[audio_idx]\n",
    "            # If you want all slices from an audio_idx\n",
    "            # to be in a single batch, just do:\n",
    "            # yield indices\n",
    "            #\n",
    "            # Otherwise, if you want them in sub-batches of size batch_size:\n",
    "            for i in range(0, len(indices), self.batch_size):\n",
    "                yield indices[i : i + self.batch_size]\n",
    "\n",
    "    def __len__(self):\n",
    "        \"\"\"\n",
    "        The total number of batches.\n",
    "        This is the sum of however many sub-batches each audio has\n",
    "        when chunked by batch_size.\n",
    "        \"\"\"\n",
    "        total_batches = 0\n",
    "        for idxs in self.audio_to_indices.values():\n",
    "            total_batches += math.ceil(len(idxs) / self.batch_size)\n",
    "        return total_batches\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "h5_3VtQAU28P"
   },
   "source": [
    "## Convolutional Neural Network Components: Conv_2d and CNN\n",
    "\n",
    "This section describes two key components of a Convolutional Neural Network (CNN)\n",
    "used for audio classification: `Conv_2d` (a convolutional block) and `CNN`\n",
    "(the overall network architecture). This network architecture is the same architecture used in the han paper.\n",
    "\n",
    "### Conv_2d: A Convolutional Block\n",
    "\n",
    "The `Conv_2d` class defines a fundamental building block for CNNs. It consists of:\n",
    "\n",
    "1. **Two Convolutional Layers:** These layers extract features from the input audio data.\n",
    "2. **LeakyReLU Activation:** Introduces non-linearity to the model, allowing it to learn\n",
    "   more complex patterns.\n",
    "3. **Max Pooling:** Reduces the spatial dimensions of the feature maps, decreasing the\n",
    "   computational complexity and making the model more robust to small variations in the input.\n",
    "4. **Dropout:** A regularization technique that helps prevent overfitting by randomly\n",
    "   ignoring some neurons during training.\n",
    "\n",
    "**Purpose:** `Conv_2d` blocks extract features, introduce non-linearity, and reduce\n",
    "spatial dimensions while preventing overfitting. By stacking multiple `Conv_2d` blocks,\n",
    "we can build deeper and more powerful CNNs.\n",
    "\n",
    "### CNN: A Convolutional Neural Network for Audio Classification\n",
    "\n",
    "The `CNN` class defines the overall architecture of the convolutional neural network.\n",
    "It utilizes multiple `Conv_2d` blocks and other layers to process audio data for\n",
    "classification tasks like instrument recognition or music genre classification.\n",
    "\n",
    "**Key Components:**\n",
    "\n",
    "1. **Multiple Conv_2d Blocks:** Extract hierarchical features from the audio data.\n",
    "2. **Additional Convolutional Layers:** Further process the extracted features.\n",
    "3. **Global Max Pooling:** Reduces the spatial dimensions to a single value per feature map.\n",
    "4. **Fully Connected Layers:** Map the learned features to the desired output classes.\n",
    "5. **Dropout:** Applied to the fully connected layers to prevent overfitting.\n",
    "\n",
    "**Purpose:** The `CNN` model learns hierarchical representations of audio data,\n",
    "starting with low-level features and progressing to more abstract features.\n",
    "The fully connected layers then use these learned features to classify the audio\n",
    "into different categories."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "P17xIcySU3iD"
   },
   "outputs": [],
   "source": [
    "\n",
    "class Conv_2d(nn.Module):\n",
    "    # shape: size of kernel / pooling: pooling factor (3 means shrinking dimension to 1/3)\n",
    "    def __init__(self, input_channels, output_channels, shape=3, padding=1, pooling=3, dropout=0.25):\n",
    "        # call constructor from nn.Module class\n",
    "        super(Conv_2d, self).__init__()\n",
    "\n",
    "        # 1st convolution layer - use input channels, output channels, add padding of 1x1\n",
    "        self.conv1 = nn.Conv2d(\n",
    "            input_channels, output_channels, shape, padding=padding)\n",
    "\n",
    "        # 2nd convolution layer - use output channels for input and output because we want number of filters to be the same\n",
    "        self.conv2 = nn.Conv2d(\n",
    "            output_channels, output_channels, shape, padding=padding)\n",
    "\n",
    "        # relu for activation\n",
    "        self.l_relu = nn.LeakyReLU(negative_slope=0.33)\n",
    "        self.maxpool = nn.MaxPool2d(pooling)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv1(x)\n",
    "        x = self.l_relu(x)\n",
    "        x = self.conv2(x)\n",
    "        x = self.l_relu(x)\n",
    "        x = self.maxpool(x)\n",
    "        x = self.dropout(x)\n",
    "        return x\n",
    "\n",
    "class CNN(nn.Module):\n",
    "  def __init__(self, input_channels=1, num_classes=11):\n",
    "    super(CNN, self).__init__()\n",
    "\n",
    "    self.layer1 = Conv_2d(input_channels, 32)\n",
    "\n",
    "    self.layer2 = Conv_2d(32, 64)\n",
    "\n",
    "    self.layer3 = Conv_2d(64, 128)\n",
    "\n",
    "    self.layer4 = nn.Conv2d(128, 256, 3, padding=1)\n",
    "\n",
    "    self.l_relu = nn.LeakyReLU(negative_slope=0.33)\n",
    "\n",
    "    self.layer5 = nn.Conv2d(256, 256, 3, padding=1)\n",
    "\n",
    "    # global max pooling takes in (x, y) parameters and shrinks remaining row x col dimension into specified numbers.\n",
    "\n",
    "    self.global_max_pool = nn.AdaptiveMaxPool2d((1, 1))\n",
    "\n",
    "    # nn.Linear -> fully connected layer. It expects flattened array as input, maps all input neurons to output neurons.\n",
    "\n",
    "    self.fc = nn.Linear(256, 1024)\n",
    "\n",
    "    self.dropout = nn.Dropout(0.5)\n",
    "\n",
    "    self.fc2 = nn.Linear(1024, num_classes)\n",
    "\n",
    "  # Used in previous experiment but not used in final paper experiment\n",
    "  def set_num_classes(self, num_classes):\n",
    "    \"\"\"Reinitialize the final layer to accommodate a different number of classes. \"\"\"\n",
    "    self.fc2 = nn.Linear(1024, num_classes)\n",
    "\n",
    "\n",
    "  def forward(self, x):\n",
    "    x = self.layer1(x)\n",
    "    x = self.layer2(x)\n",
    "    x = self.layer3(x)\n",
    "    x = self.layer4(x)\n",
    "    x = self.l_relu(x)\n",
    "    x = self.layer5(x)\n",
    "    x = self.l_relu(x)\n",
    "    x = self.global_max_pool(x)\n",
    "\n",
    "\n",
    "    x = x.view(x.size(0), -1)\n",
    "    x = self.fc(x)\n",
    "    x = self.dropout(x)\n",
    "    x = self.fc2(x)\n",
    "\n",
    "    # nn.BCEWithLogitsLoss() already uses sigmoid internally, so we don't define it in our model\n",
    "    # x = F.sigmoid(x)\n",
    "\n",
    "    return x\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training / Testing\n",
    "This script implements a systematic training pipeline for the CNN model. It was originally used to fine-tune a model after pretraining, but one can easily change the layer configuration to just train from scratch. We ended up training models from scratch using this code. This code allows testing on different layer freezing configurations across multiple seeds. After training, the model is evaluated on the IRMAS test dataset, and the results are saved.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 88
    },
    "id": "aWKAuKUNVvLC",
    "outputId": "ed4ef2b1-3693-4e54-8dda-6f65d8264faa"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "  <style>\n",
       "    pre {\n",
       "        white-space: pre-wrap;\n",
       "    }\n",
       "  </style>\n",
       "  "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "======================================================================\n",
      "RUNNING EXPERIMENTS WITH SEED 1\n",
      "======================================================================\n"
     ]
    }
   ],
   "source": [
    "#@title Training / Testing\n",
    "\n",
    "\n",
    "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "# ----------------------------\n",
    "# 1) Define layer freezing configurations\n",
    "# ----------------------------\n",
    "layer_configs = [\n",
    "    # {\n",
    "    #     \"name\": \"freeze_all_conv\",\n",
    "    #     \"layers\": [\"layer1\", \"layer2\", \"layer3\", \"layer4\", \"layer5\"],\n",
    "    #     \"description\": \"Freeze all convolutional layers\"\n",
    "    # },\n",
    "    # {\n",
    "    #     \"name\": \"freeze_early_mid\",\n",
    "    #     \"layers\": [\"layer1\", \"layer2\", \"layer3\"],\n",
    "    #     \"description\": \"Freeze early and mid convolutional layers\"\n",
    "    # },\n",
    "    # {\n",
    "    #     \"name\": \"freeze_early\",\n",
    "    #     \"layers\": [\"layer1\", \"layer2\"],\n",
    "    #     \"description\": \"Freeze only early convolutional layers\"\n",
    "    # },\n",
    "    # {\n",
    "    #     \"name\": \"freeze_first\",\n",
    "    #     \"layers\": [\"layer1\"],\n",
    "    #     \"description\": \"Freeze only first convolutional layer\"\n",
    "    # },\n",
    "    {\n",
    "        \"name\": \"unfreeze_all\",\n",
    "        \"layers\": [],\n",
    "        \"description\": \"Fine-tune all layers\"\n",
    "    }\n",
    "]\n",
    "\n",
    "# List of seeds to test\n",
    "seeds = [1, 10, 9000]  # Modify as needed\n",
    "\n",
    "# ----------------------------\n",
    "# 2) Path configuration\n",
    "# ----------------------------\n",
    "# Base directory for fine-tuning results\n",
    "finetune_base_dir = '/content/drive/MyDrive/Aiden/genre_auxiliary_experiment/IRMASv9_all_features'\n",
    "os.makedirs(finetune_base_dir, exist_ok=True)\n",
    "\n",
    "# Path to pretrained model - use the exact folder name\n",
    "pretrain_base_dir = '/content/drive/MyDrive/Aiden/genre_auxiliary_experiment/pretrain_openmic'\n",
    "pretrain_folder_name = \"seed_4_openmic_only_100epochs\"  # Change this to your pretrained model folder\n",
    "\n",
    "# ----------------------------\n",
    "# 3) Ensure worker seeds are consistent for reproducibility\n",
    "# ----------------------------\n",
    "def seed_worker(worker_id):\n",
    "    worker_seed = torch.initial_seed() % 2**32\n",
    "    np.random.seed(worker_seed)\n",
    "    random.seed(worker_seed)\n",
    "\n",
    "\n",
    "# ----------------------------\n",
    "# 4) Initialize result tracking\n",
    "# ----------------------------\n",
    "all_results = {\n",
    "    \"configs\": [config[\"name\"] for config in layer_configs],\n",
    "    \"seeds\": seeds,\n",
    "    \"f1_macro\": {},\n",
    "    \"f1_micro\": {},\n",
    "    \"val_accuracy\": {}\n",
    "}\n",
    "\n",
    "# ----------------------------\n",
    "# 5) Main experiment loop\n",
    "# ----------------------------\n",
    "for SEED in seeds:\n",
    "    print(f\"\\n{'='*70}\")\n",
    "    print(f\"RUNNING EXPERIMENTS WITH SEED {SEED}\")\n",
    "    print(f\"{'='*70}\")\n",
    "\n",
    "    # Set seed for reproducibility\n",
    "    random.seed(SEED)\n",
    "    np.random.seed(SEED)\n",
    "    torch.manual_seed(SEED)\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.manual_seed(SEED)\n",
    "        torch.cuda.manual_seed_all(SEED)\n",
    "        torch.backends.cudnn.deterministic = True\n",
    "        torch.backends.cudnn.benchmark = False\n",
    "\n",
    "    # Initialize tracking dictionary for this seed\n",
    "    seed_results = {\n",
    "        \"config_names\": [],\n",
    "        \"val_accuracies\": [],\n",
    "        \"f1_macro\": [],\n",
    "        \"f1_micro\": [],\n",
    "        \"early_stop_epoch\": []\n",
    "    }\n",
    "\n",
    "    # Get pretrained model path\n",
    "    pretrained_model_path = os.path.join(pretrain_base_dir, pretrain_folder_name, \"best_model.pth\")\n",
    "\n",
    "    # Create datasets with this seed\n",
    "    train_dataset = OpenPIRDataset(\n",
    "        data_file=\"/content/drive/MyDrive/Data/IRMAS/IRMASG_Precomputed_Features_v9.pkl\",\n",
    "        split='train', seed=SEED, openmic=False  \n",
    "    )\n",
    "\n",
    "    valid_dataset = OpenPIRDataset(\n",
    "        data_file=\"/content/drive/MyDrive/Data/IRMAS/IRMASG_Precomputed_Features_v9.pkl\",\n",
    "        split='valid', seed=SEED, openmic=False\n",
    "    )\n",
    "\n",
    "    # # Compute normalization parameters from training data\n",
    "    # train_mean, train_std = compute_mean_std(train_dataset)\n",
    "\n",
    "    # # Apply normalization to datasets\n",
    "    # train_dataset.mean = train_mean\n",
    "    # train_dataset.std = train_std\n",
    "    # valid_dataset.mean = train_mean\n",
    "    # valid_dataset.std = train_std\n",
    "\n",
    "    # Create test dataset with same normalization\n",
    "    test_dataset = IRMASGTestDataset(\n",
    "        data_file=\"/content/drive/MyDrive/irmas_test_set_augmented_features.pkl\",\n",
    "        split='test', test_sample_half=False, seed=SEED,\n",
    "        # mean=train_mean, std=train_std\n",
    "    )\n",
    "\n",
    "    # Build audio index mapping for test data\n",
    "    audio_to_indices = defaultdict(list)\n",
    "    for i in range(len(test_dataset)):\n",
    "        _, _, audio_idx = test_dataset[i]\n",
    "        audio_to_indices[audio_idx].append(i)\n",
    "\n",
    "    max_slices = max(len(idxs) for idxs in audio_to_indices.values())\n",
    "    batch_size = max_slices\n",
    "\n",
    "    # Create data loaders\n",
    "    g = torch.Generator()\n",
    "    g.manual_seed(SEED)\n",
    "\n",
    "    train_loader = DataLoader(\n",
    "        train_dataset, batch_size=64,\n",
    "        shuffle=True, num_workers=8,\n",
    "        worker_init_fn=seed_worker, generator=g\n",
    "    )\n",
    "\n",
    "    valid_loader = DataLoader(\n",
    "        valid_dataset, batch_size=64,\n",
    "        shuffle=False, num_workers=8,\n",
    "        worker_init_fn=seed_worker, generator=g\n",
    "    )\n",
    "\n",
    "    test_batch_sampler = TestBatchSampler(audio_to_indices, batch_size=batch_size)\n",
    "    test_loader = DataLoader(\n",
    "        test_dataset, batch_sampler=test_batch_sampler,\n",
    "        num_workers=8, pin_memory=True\n",
    "    )\n",
    "\n",
    "    # Loop through each layer freezing configuration\n",
    "    for config in layer_configs:\n",
    "        config_name = config[\"name\"]\n",
    "        freeze_layers = config[\"layers\"]\n",
    "\n",
    "        print(f\"\\n{'-'*60}\")\n",
    "        print(f\"Testing configuration: {config_name}\")\n",
    "        print(f\"Description: {config['description']}\")\n",
    "        print(f\"Freezing layers: {', '.join(freeze_layers) if freeze_layers else 'None'}\")\n",
    "        print(f\"{'-'*60}\")\n",
    "\n",
    "        # Create experiment directory for this seed+config combination\n",
    "        experiment_dir = os.path.join(finetune_base_dir, f'seed_{SEED}_{config_name}')\n",
    "        os.makedirs(experiment_dir, exist_ok=True)\n",
    "\n",
    "        # Log configuration details\n",
    "        with open(os.path.join(experiment_dir, \"config.txt\"), \"w\") as f:\n",
    "            f.write(f\"Seed: {SEED}\\n\")\n",
    "            f.write(f\"Configuration: {config_name}\\n\")\n",
    "            f.write(f\"Description: {config['description']}\\n\")\n",
    "            f.write(f\"Frozen layers: {', '.join(freeze_layers) if freeze_layers else 'None'}\\n\")\n",
    "            f.write(f\"Pretrained model: {pretrained_model_path}\\n\")\n",
    "            f.write(f\"Pretrained from folder: {pretrain_folder_name}\\n\")\n",
    "\n",
    "        # Load pretrained model (trained on OpenMIC with 10 classes)\n",
    "        cnn = CNN(input_channels=1, num_classes=11).to(device)\n",
    "        # cnn.load_state_dict(torch.load(pretrained_model_path, map_location=device))\n",
    "        # cnn.set_num_classes(11)\n",
    "        # cnn = cnn.to(device)\n",
    "\n",
    "\n",
    "\n",
    "        # Apply freezing based on configuration\n",
    "        for name, param in cnn.named_parameters():\n",
    "            # Extract layer name (before the first dot)\n",
    "            layer_name = name.split('.')[0]\n",
    "\n",
    "            # Set requires_grad based on whether this layer should be frozen\n",
    "            param.requires_grad = layer_name not in freeze_layers\n",
    "\n",
    "            # Log the freezing status\n",
    "            status = \"frozen (fixed)\" if layer_name in freeze_layers else \"trainable\"\n",
    "            print(f\"  {name}: {status}\")\n",
    "\n",
    "        # Configure optimizer (only for trainable parameters)\n",
    "        optimizer = optim.Adam(\n",
    "            filter(lambda p: p.requires_grad, cnn.parameters()),\n",
    "            lr=0.001,\n",
    "            weight_decay=1e-4\n",
    "        )\n",
    "\n",
    "        # Learning rate scheduler\n",
    "        scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(\n",
    "            optimizer, mode='min', factor=0.5, patience=5, min_lr=1e-6, verbose=True\n",
    "        )\n",
    "\n",
    "        # Set up loss function\n",
    "        criterion_instr = torch.nn.BCEWithLogitsLoss()\n",
    "\n",
    "        # Early stopping setup\n",
    "        early_stop_patience = 15\n",
    "        early_stop_counter = 0\n",
    "\n",
    "        # Training metrics\n",
    "        train_losses = []\n",
    "        val_losses = []\n",
    "        train_accuracies = []\n",
    "        val_accuracies = []\n",
    "        best_val_loss = float('inf')\n",
    "        stopped_epoch = 100  # Default to max epochs\n",
    "        num_epochs = 100     # Fine-tuning typically needs fewer epochs\n",
    "\n",
    "        # Main training loop for this configuration\n",
    "        for epoch in tqdm(range(num_epochs), desc=f\"Training {config_name}\"):\n",
    "            # Training phase\n",
    "            cnn.train()\n",
    "            running_loss = 0.0\n",
    "            train_correct = 0\n",
    "            train_total = 0\n",
    "\n",
    "            for data, labels, aux_labels in train_loader:\n",
    "                data = data.to(device)\n",
    "                labels = labels.to(device)\n",
    "\n",
    "                # Convert to one-hot for BCE loss\n",
    "                BCE_labels = labels.float()\n",
    "\n",
    "                optimizer.zero_grad()\n",
    "                outputs = cnn(data)\n",
    "                loss = criterion_instr(outputs, BCE_labels)\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "\n",
    "                # Track metrics\n",
    "                # running_loss += loss.item()\n",
    "                # predicted_class = torch.argmax(outputs, dim=1)\n",
    "                # train_correct += (predicted_class == labels).sum().item()\n",
    "                # train_total += labels.size(0)\n",
    "\n",
    "                # After loss.backward() and optimizer.step()\n",
    "                running_loss += loss.item()\n",
    "\n",
    "                # Apply sigmoid to convert logits to probabilities\n",
    "                probabilities = torch.sigmoid(outputs)\n",
    "\n",
    "                # Threshold probabilities at 0.5 to get predicted labels\n",
    "                predicted_labels = (probabilities >= 0.5).int()\n",
    "\n",
    "                # Compute accuracy for multi-label classification\n",
    "                correct_predictions = (predicted_labels == labels.int()).float()\n",
    "                accuracy_per_sample = correct_predictions.mean(dim=1)  # accuracy per sample\n",
    "                batch_accuracy = accuracy_per_sample.mean().item()\n",
    "\n",
    "                train_correct += batch_accuracy * labels.size(0)\n",
    "                train_total += labels.size(0)\n",
    "\n",
    "            # Calculate epoch metrics\n",
    "            avg_train_loss = running_loss / len(train_loader)\n",
    "            train_accuracy = train_correct / train_total\n",
    "            train_losses.append(avg_train_loss)\n",
    "            train_accuracies.append(train_accuracy)\n",
    "\n",
    "            # Validation phase\n",
    "            cnn.eval()\n",
    "            val_loss = 0.0\n",
    "            val_correct = 0\n",
    "            val_total = 0\n",
    "\n",
    "            with torch.no_grad():\n",
    "                for data, labels, aux_labels in valid_loader:\n",
    "                    data = data.to(device)\n",
    "                    labels = labels.to(device)\n",
    "                    BCE_labels = labels.float()\n",
    "\n",
    "                    outputs = cnn(data)\n",
    "                    loss = criterion_instr(outputs, BCE_labels)\n",
    "                    val_loss += loss.item()\n",
    "\n",
    "                    # predicted_class = torch.argmax(outputs, dim=1)\n",
    "                    # val_correct += (predicted_class == labels).sum().item()\n",
    "                    # val_total += labels.size(0)\n",
    "\n",
    "                    probabilities = torch.sigmoid(outputs)\n",
    "                    predicted_labels = (probabilities >= 0.5).int()\n",
    "\n",
    "                    correct_predictions = (predicted_labels == labels.int()).float()\n",
    "                    accuracy_per_sample = correct_predictions.mean(dim=1)\n",
    "                    batch_accuracy = accuracy_per_sample.mean().item()\n",
    "\n",
    "                    val_correct += batch_accuracy * labels.size(0)\n",
    "                    val_total += labels.size(0)\n",
    "\n",
    "\n",
    "\n",
    "            # Calculate validation metrics\n",
    "            avg_val_loss = val_loss / len(valid_loader)\n",
    "            val_accuracy = val_correct / val_total\n",
    "            val_losses.append(avg_val_loss)\n",
    "            val_accuracies.append(val_accuracy)\n",
    "\n",
    "            # Update learning rate scheduler\n",
    "            scheduler.step(avg_val_loss)\n",
    "            current_lr = optimizer.param_groups[0][\"lr\"]\n",
    "\n",
    "            # Log epoch results\n",
    "            with open(os.path.join(experiment_dir, \"training_log.txt\"), \"a\") as f:\n",
    "                f.write(f\"Epoch {epoch+1}/{num_epochs}, Train Loss: {avg_train_loss:.4f}, \"\n",
    "                        f\"Train Acc: {train_accuracy:.4f}, Val Loss: {avg_val_loss:.4f}, \"\n",
    "                        f\"Val Acc: {val_accuracy:.4f}, LR: {current_lr:.6f}\\n\")\n",
    "\n",
    "            # Save best model and check for early stopping\n",
    "            if avg_val_loss < best_val_loss:\n",
    "                best_val_loss = avg_val_loss\n",
    "                torch.save(cnn.state_dict(), os.path.join(experiment_dir, \"best_model.pth\"))\n",
    "                early_stop_counter = 0\n",
    "            else:\n",
    "                early_stop_counter += 1\n",
    "\n",
    "            # Check for early stopping\n",
    "            if early_stop_counter >= early_stop_patience:\n",
    "                print(f\"Early stopping at epoch {epoch+1}\")\n",
    "                with open(os.path.join(experiment_dir, \"training_log.txt\"), \"a\") as f:\n",
    "                    f.write(f\"Early stopping triggered at epoch {epoch+1}\\n\")\n",
    "                stopped_epoch = epoch + 1\n",
    "                break\n",
    "\n",
    "        # Save training curves\n",
    "        plt.figure(figsize=(12, 5))\n",
    "        plt.subplot(1, 2, 1)\n",
    "        plt.plot(train_losses, label='Training Loss')\n",
    "        plt.plot(val_losses, label='Validation Loss')\n",
    "        plt.xlabel('Epochs')\n",
    "        plt.ylabel('Loss')\n",
    "        plt.title(f'Loss Curves - Seed {SEED}, {config_name}')\n",
    "        plt.legend()\n",
    "\n",
    "        plt.subplot(1, 2, 2)\n",
    "        plt.plot(train_accuracies, label='Training Accuracy')\n",
    "        plt.plot(val_accuracies, label='Validation Accuracy')\n",
    "        plt.xlabel('Epochs')\n",
    "        plt.ylabel('Accuracy')\n",
    "        plt.title(f'Accuracy Curves - Seed {SEED}, {config_name}')\n",
    "        plt.legend()\n",
    "\n",
    "        plt.tight_layout()\n",
    "        plt.savefig(os.path.join(experiment_dir, 'training_curves.png'))\n",
    "        plt.close()\n",
    "\n",
    "        # Test best model on the test set\n",
    "        best_model_path = os.path.join(experiment_dir, \"best_model.pth\")\n",
    "        cnn.load_state_dict(torch.load(best_model_path))\n",
    "\n",
    "        cnn.eval()\n",
    "        final_predictions = []\n",
    "        final_labels = []\n",
    "\n",
    "        with open(os.path.join(experiment_dir, \"predictions.txt\"), \"w\") as pred_file:\n",
    "            pred_file.write(\"Batch-wise Model Predictions vs. Actual Labels\\n\")\n",
    "            pred_file.write(\"=\" * 80 + \"\\n\")\n",
    "\n",
    "            with torch.no_grad():\n",
    "                for data, labels, audio_idxs in test_loader:\n",
    "                    data = data.to(device)\n",
    "                    true_label = labels[0].int().numpy()\n",
    "\n",
    "                    outputs = cnn(data)\n",
    "                    probabilities = torch.sigmoid(outputs)\n",
    "\n",
    "                    # Sum across all slices for this audio\n",
    "                    sum_prob = torch.sum(probabilities, dim=0)\n",
    "\n",
    "                    # Normalize the sum\n",
    "                    max_sum = torch.max(sum_prob)\n",
    "                    if max_sum > 0:  # Avoid division by zero\n",
    "                        normalized_sum_prob = sum_prob / max_sum\n",
    "                    else:\n",
    "                        normalized_sum_prob = sum_prob\n",
    "\n",
    "                    # Apply threshold\n",
    "                    predicted = (normalized_sum_prob.cpu() >= 0.5).int().numpy()\n",
    "\n",
    "                    final_predictions.append(predicted)\n",
    "                    final_labels.append(true_label)\n",
    "\n",
    "                    # Write predictions to file\n",
    "                    pred_file.write(f\"Audio ID: {audio_idxs[0].item()}\\n\")\n",
    "                    pred_file.write(f\"Predicted: {predicted.tolist()}\\n\")\n",
    "                    pred_file.write(f\"Actual: {true_label.tolist()}\\n\")\n",
    "                    pred_file.write(\"-\" * 80 + \"\\n\")\n",
    "\n",
    "        # Calculate metrics\n",
    "        final_predictions = np.array(final_predictions)\n",
    "        final_labels = np.array(final_labels)\n",
    "\n",
    "        f1_macro = f1_score(final_labels, final_predictions, average='macro')\n",
    "        f1_micro = f1_score(final_labels, final_predictions, average='micro')\n",
    "        hl = hamming_loss(final_labels, final_predictions)\n",
    "\n",
    "        print(f\"\\nResults for seed {SEED}, config {config_name}:\")\n",
    "        print(f\"F1 Macro: {f1_macro:.4f}\")\n",
    "        print(f\"F1 Micro: {f1_micro:.4f}\")\n",
    "        print(f\"Hamming Loss: {hl:.4f}\")\n",
    "\n",
    "        # Save results\n",
    "        with open(os.path.join(experiment_dir, \"results.txt\"), \"w\") as f:\n",
    "            f.write(f\"Seed: {SEED}\\n\")\n",
    "            f.write(f\"Configuration: {config_name}\\n\")\n",
    "            f.write(f\"F1 Macro: {f1_macro:.4f}\\n\")\n",
    "            f.write(f\"F1 Micro: {f1_micro:.4f}\\n\")\n",
    "            f.write(f\"Hamming Loss: {hl:.4f}\\n\")\n",
    "            f.write(f\"Best Validation Loss: {best_val_loss:.4f}\\n\")\n",
    "            f.write(f\"Best Validation Accuracy: {max(val_accuracies):.4f}\\n\")\n",
    "            f.write(f\"Stopped at epoch: {stopped_epoch}/{num_epochs}\\n\")\n",
    "\n",
    "        # Generate classification report\n",
    "        report_str = classification_report(final_labels, final_predictions)\n",
    "        with open(os.path.join(experiment_dir, \"classification_report.txt\"), \"w\") as f:\n",
    "            f.write(f\"F1 Macro: {f1_macro:.4f}\\n\")\n",
    "            f.write(f\"F1 Micro: {f1_micro:.4f}\\n\")\n",
    "            f.write(f\"Hamming Loss: {hl:.4f}\\n\\n\")\n",
    "            f.write(report_str)\n",
    "\n",
    "        # Store results for this seed+config\n",
    "        seed_results[\"config_names\"].append(config_name)\n",
    "        seed_results[\"val_accuracies\"].append(max(val_accuracies))\n",
    "        seed_results[\"f1_macro\"].append(f1_macro)\n",
    "        seed_results[\"f1_micro\"].append(f1_micro)\n",
    "        seed_results[\"early_stop_epoch\"].append(stopped_epoch)\n",
    "\n",
    "    # Create comparative visualization for this seed\n",
    "    plt.figure(figsize=(14, 8))\n",
    "\n",
    "    # Plot F1 scores\n",
    "    plt.subplot(2, 1, 1)\n",
    "    plt.bar(np.arange(len(seed_results[\"config_names\"])) - 0.15,\n",
    "            seed_results[\"f1_macro\"], width=0.3, label=\"F1 Macro\")\n",
    "    plt.bar(np.arange(len(seed_results[\"config_names\"])) + 0.15,\n",
    "            seed_results[\"f1_micro\"], width=0.3, label=\"F1 Micro\")\n",
    "    plt.xticks(range(len(seed_results[\"config_names\"])), seed_results[\"config_names\"], rotation=45)\n",
    "    plt.ylabel(\"F1 Score\")\n",
    "    plt.title(f\"F1 Scores Across Layer Freezing Configurations (Seed {SEED})\")\n",
    "    plt.legend()\n",
    "\n",
    "    # Plot validation accuracies\n",
    "    plt.subplot(2, 1, 2)\n",
    "    plt.bar(range(len(seed_results[\"config_names\"])), seed_results[\"val_accuracies\"])\n",
    "    plt.xticks(range(len(seed_results[\"config_names\"])), seed_results[\"config_names\"], rotation=45)\n",
    "    plt.ylabel(\"Best Validation Accuracy\")\n",
    "    plt.title(f\"Validation Accuracy Across Layer Freezing Configurations (Seed {SEED})\")\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(os.path.join(finetune_base_dir, f\"seed_{SEED}_comparison.png\"))\n",
    "    plt.close()\n",
    "\n",
    "    # Save seed results to json\n",
    "    with open(os.path.join(finetune_base_dir, f\"seed_{SEED}_results.json\"), \"w\") as f:\n",
    "        json.dump(seed_results, f, indent=4)\n",
    "\n",
    "    # Store in all_results\n",
    "    all_results[\"f1_macro\"][SEED] = seed_results[\"f1_macro\"]\n",
    "    all_results[\"f1_micro\"][SEED] = seed_results[\"f1_micro\"]\n",
    "    all_results[\"val_accuracy\"][SEED] = seed_results[\"val_accuracies\"]\n",
    "\n",
    "# After all seeds, compute average performance and standard deviation for each configuration\n",
    "avg_f1_macro = []\n",
    "std_f1_macro = []\n",
    "avg_f1_micro = []\n",
    "std_f1_micro = []\n",
    "avg_val_acc = []\n",
    "std_val_acc = []\n",
    "\n",
    "for i, config in enumerate([config[\"name\"] for config in layer_configs]):\n",
    "    # Collect metrics across seeds for this configuration\n",
    "    f1_macro_values = [all_results[\"f1_macro\"][seed][i] for seed in seeds]\n",
    "    f1_micro_values = [all_results[\"f1_micro\"][seed][i] for seed in seeds]\n",
    "    val_acc_values = [all_results[\"val_accuracy\"][seed][i] for seed in seeds]\n",
    "\n",
    "    # Compute statistics\n",
    "    avg_f1_macro.append(np.mean(f1_macro_values))\n",
    "    std_f1_macro.append(np.std(f1_macro_values))\n",
    "    avg_f1_micro.append(np.mean(f1_micro_values))\n",
    "    std_f1_micro.append(np.std(f1_micro_values))\n",
    "    avg_val_acc.append(np.mean(val_acc_values))\n",
    "    std_val_acc.append(np.std(val_acc_values))\n",
    "\n",
    "# Add averages and standard deviations to results\n",
    "all_results[\"avg_f1_macro\"] = avg_f1_macro\n",
    "all_results[\"std_f1_macro\"] = std_f1_macro\n",
    "all_results[\"avg_f1_micro\"] = avg_f1_micro\n",
    "all_results[\"std_f1_micro\"] = std_f1_micro\n",
    "all_results[\"avg_val_accuracy\"] = avg_val_acc\n",
    "all_results[\"std_val_accuracy\"] = std_val_acc\n",
    "\n",
    "# Save overall results\n",
    "with open(os.path.join(finetune_base_dir, \"overall_results.json\"), \"w\") as f:\n",
    "    json.dump(all_results, f, indent=4)\n",
    "\n",
    "# Create final visualization with error bars\n",
    "plt.figure(figsize=(14, 8))\n",
    "x = np.arange(len(layer_configs))\n",
    "width = 0.35\n",
    "\n",
    "plt.bar(x - width/2, avg_f1_macro, width,\n",
    "        yerr=std_f1_macro, capsize=5, label=\"F1 Macro\", color='blue', alpha=0.7)\n",
    "plt.bar(x + width/2, avg_f1_micro, width,\n",
    "        yerr=std_f1_micro, capsize=5, label=\"F1 Micro\", color='green', alpha=0.7)\n",
    "\n",
    "plt.xlabel(\"Layer Freezing Configuration\")\n",
    "plt.ylabel(\"F1 Score (Average over Seeds)\")\n",
    "plt.title(\"Performance Comparison Across Layer Freezing Strategies\")\n",
    "plt.xticks(x, [config[\"name\"] for config in layer_configs], rotation=45, ha=\"right\")\n",
    "plt.legend()\n",
    "plt.grid(True, linestyle='--', alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.savefig(os.path.join(finetune_base_dir, \"overall_comparison.png\"))\n",
    "plt.close()\n",
    "\n",
    "print(f\"\\nAll experiments complete! Results saved to {finetune_base_dir}\")\n"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "A100",
   "machine_shape": "hm",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
